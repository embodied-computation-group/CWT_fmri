---
title: "CWT fMRI Study - GLMM Analysis Report"
subtitle: "Predictive Processing in Emotion Recognition"
author: "CWT fMRI Analysis Team"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
    code_folding: hide
    fig_width: 10
    fig_height: 6
    fig_caption: true
    df_print: kable
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(lme4)
library(knitr)
library(kableExtra)

# Set global options
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  dpi = 300
)

# Load pre-estimated models
accuracy_model <- readRDS("../results/models/accuracy_model_simple.rds")
choice_model <- readRDS("../results/models/choice_model_simple.rds")
rt_model <- readRDS("../results/models/rt_model_simple.rds")
confidence_model <- readRDS("../results/models/confidence_model_simple.rds")

# Function to extract model statistics
extract_model_stats <- function(model, model_name) {
  summary_obj <- summary(model)
  coef_table <- as.data.frame(summary_obj$coefficients$cond)
  coef_table$Parameter <- rownames(coef_table)
  
  # Add significance stars
  coef_table$Significance <- case_when(
    coef_table$`Pr(>|z|)` < 0.001 ~ "***",
    coef_table$`Pr(>|z|)` < 0.01 ~ "**",
    coef_table$`Pr(>|z|)` < 0.05 ~ "*",
    coef_table$`Pr(>|z|)` < 0.1 ~ ".",
    TRUE ~ ""
  )
  
  # Format for display
  coef_table$Estimate <- round(coef_table$Estimate, 4)
  coef_table$`Std. Error` <- round(coef_table$`Std. Error`, 4)
  coef_table$`z value` <- round(coef_table$`z value`, 3)
  coef_table$`Pr(>|z|)` <- format(coef_table$`Pr(>|z|)`, scientific = TRUE, digits = 3)
  
  return(coef_table)
}

# Function to create fit statistics table
create_fit_stats <- function() {
  models <- list(
    "Accuracy Model" = accuracy_model,
    "Choice Model" = choice_model,
    "Response Time Model" = rt_model,
    "Confidence Model" = confidence_model
  )
  
  fit_stats <- data.frame(
    Model = names(models),
    AIC = sapply(models, AIC),
    BIC = sapply(models, BIC),
    LogLik = sapply(models, logLik),
    N_Observations = sapply(models, nobs),
    N_Subjects = sapply(models, function(m) length(ranef(m)$SubNo$`(Intercept)`))
  )
  
  fit_stats$AIC <- round(fit_stats$AIC, 2)
  fit_stats$BIC <- round(fit_stats$BIC, 2)
  fit_stats$LogLik <- round(fit_stats$LogLik, 2)
  
  return(fit_stats)
}
```

## Study Overview

This report presents the results of four Generalized Linear Mixed Models (GLMMs) examining predictive processing in emotion recognition using a confidence weighting task.

**Participants:** 202 subjects  
**Total Trials:** 53,592 (48,199 after filtering)  
**Task:** Predict emotional faces (Happy/Angry) based on visual cues in a reversal learning paradigm

### Experimental Design

- **Trial Validity:** Valid vs Invalid vs Non-predictive trials
- **Stimulus Noise:** High noise (ambiguous) vs Low noise (clear) faces
- **Learning:** Trials since reversal (learning dynamics)
- **Face Emotion:** Happy vs Angry faces

---

## Model Results

### 1. Accuracy Model (High Noise Trials Only)

**Research Question:** How does trial validity and learning affect accuracy in high-noise trials?

```{r accuracy-stats}
accuracy_stats <- extract_model_stats(accuracy_model, "Accuracy Model")
accuracy_stats %>%
  kable(
    caption = "Accuracy Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings:**
- ‚úÖ **Trial Validity:** Valid trials show significantly higher accuracy (z = `r accuracy_stats$`z value`[accuracy_stats$Parameter == "TrialValidity2_numeric"]`, p < 0.001)
- ‚ùå **Face Emotion:** Happy faces show lower accuracy than angry faces (z = `r accuracy_stats$`z value`[accuracy_stats$Parameter == "FaceEmotHappy"]`, p < 0.001)
- üîÑ **Learning:** Trial validity effects change with learning (interaction: z = `r accuracy_stats$`z value`[accuracy_stats$Parameter == "TrialValidity2_numeric:TrialsSinceRev_scaled"]`, p = `r accuracy_stats$`Pr(>|z|)`[accuracy_stats$Parameter == "TrialValidity2_numeric:TrialsSinceRev_scaled"]`)

**Interpretation:** Participants are more accurate when cues correctly predict the face emotion, but this effect changes over time as they learn the task contingencies.

```{r accuracy-plot, fig.cap="Accuracy Model Predictions: Trial Validity √ó Trials Since Reversal √ó Face Emotion"}
knitr::include_graphics("../results/figures/glmm_models/glmm_accuracy_model.png")
```

---

### 2. Choice Model (High Noise Trials Only)

**Research Question:** How do signaled faces and actual emotions influence choice behavior?

```{r choice-stats}
choice_stats <- extract_model_stats(choice_model, "Choice Model")
choice_stats %>%
  kable(
    caption = "Choice Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings:**
- üéØ **Signaled Face:** Angry signaled faces reduce choice of angry (z = `r choice_stats$`z value`[choice_stats$Parameter == "SignaledFaceAngry"]`, p < 0.001)
- üòä **Actual Emotion:** Happy faces strongly predict happy choices (z = `r choice_stats$`z value`[choice_stats$Parameter == "FaceEmotHappy"]`, p < 0.001)
- üîÑ **Learning:** Learning effects interact with signaled face (z = `r choice_stats$`z value`[choice_stats$Parameter == "SignaledFaceAngry:TrialsSinceRev_scaled"]`, p < 0.001)

**Interpretation:** Participants use predictive cues to guide their choices, but also respond strongly to the actual face emotion. Learning modulates these effects.

```{r choice-plot, fig.cap="Choice Model Predictions: Signaled Face √ó Face Emotion √ó Trials Since Reversal"}
knitr::include_graphics("../results/figures/glmm_models/glmm_choice_model.png")
```

---

### 3. Response Time Model (All Trials)

**Research Question:** How do stimulus noise and trial validity affect response times?

```{r rt-stats}
rt_stats <- extract_model_stats(rt_model, "Response Time Model")
rt_stats %>%
  kable(
    caption = "Response Time Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings:**
- ‚è±Ô∏è **Stimulus Noise:** High noise trials show significantly longer RTs (z = `r rt_stats$`z value`[rt_stats$Parameter == "StimNoisehigh noise"]`, p < 0.001)
- ‚ö° **Trial Validity:** Invalid trials show shorter RTs (z = `r rt_stats$`z value`[rt_stats$Parameter == "TrialValidity2_numeric"]`, p < 0.001)
- üîÑ **Learning:** Validity effects change with learning (interaction: z = `r rt_stats$`z value`[rt_stats$Parameter == "TrialValidity2_numeric:TrialsSinceRev_scaled"]`, p = `r rt_stats$`Pr(>|z|)`[rt_stats$Parameter == "TrialValidity2_numeric:TrialsSinceRev_scaled"]`)

**Interpretation:** Task difficulty (noise) increases response times, while invalid trials (surprising outcomes) lead to faster responses, possibly due to surprise or reduced confidence.

```{r rt-plot, fig.cap="Response Time Model Predictions: Stimulus Noise √ó Trial Validity √ó Trials Since Reversal"}
knitr::include_graphics("../results/figures/glmm_models/glmm_rt_model.png")
```

---

### 4. Confidence Model (All Trials)

**Research Question:** How do trial validity and stimulus noise affect confidence ratings?

```{r confidence-stats}
confidence_stats <- extract_model_stats(confidence_model, "Confidence Model")
confidence_stats %>%
  kable(
    caption = "Confidence Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings:**
- üò∞ **Stimulus Noise:** High noise trials show significantly lower confidence (z = `r confidence_stats$`z value`[confidence_stats$Parameter == "StimNoisehigh noise"]`, p < 0.001)
- üí™ **Trial Validity:** Valid trials show higher confidence (z = `r confidence_stats$`z value`[confidence_stats$Parameter == "TrialValidity2_numeric"]`, p = `r confidence_stats$`Pr(>|z|)`[confidence_stats$Parameter == "TrialValidity2_numeric"]`)
- üòä **Face Emotion:** Happy faces show higher confidence (z = `r confidence_stats$`z value`[confidence_stats$Parameter == "FaceEmotHappy"]`, p < 0.001)
- üîÑ **Learning:** Validity effects change with learning (interaction: z = `r confidence_stats$`z value`[confidence_stats$Parameter == "TrialValidity2_numeric:TrialsSinceRev_scaled"]`, p < 0.001)

**Interpretation:** Participants are less confident when faces are ambiguous (high noise) and more confident when cues correctly predict outcomes. Happy faces generally elicit higher confidence.

```{r confidence-plot, fig.cap="Confidence Model Predictions: Trial Validity √ó Stimulus Noise √ó Trials Since Reversal"}
knitr::include_graphics("../results/figures/glmm_models/glmm_confidence_model.png")
```

---

## Model Comparison

```{r model-comparison}
fit_stats <- create_fit_stats()
fit_stats %>%
  kable(
    caption = "Model Fit Statistics",
    col.names = c("Model", "AIC", "BIC", "Log Likelihood", "N Observations", "N Subjects"),
    align = c("l", "r", "r", "r", "r", "r")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  )
```

---

## Key Findings Summary

### Main Effects

| Variable | Accuracy | Choice | Response Time | Confidence |
|----------|----------|--------|---------------|------------|
| **Trial Validity** | ‚úÖ Higher for valid | ‚úÖ Influences choice | ‚ö° Faster for invalid | üí™ Higher for valid |
| **Stimulus Noise** | - | - | ‚è±Ô∏è Slower for high noise | üò∞ Lower for high noise |
| **Face Emotion** | ‚ùå Lower for happy | üòä Strong preference | - | üòä Higher for happy |
| **Learning** | üîÑ Modulates validity | üîÑ Modulates choice | üîÑ Modulates validity | üîÑ Modulates validity |

### Interaction Effects

- **Validity √ó Learning:** Trial validity effects change with learning across all models
- **Noise √ó Validity:** Noise effects interact with trial validity in RT and confidence
- **Emotion √ó Validity:** Different patterns for happy vs angry faces

---

## Conclusions

This analysis reveals robust evidence for **predictive processing** in emotion recognition:

1. **Trial validity** consistently affects all dependent measures
2. **Stimulus noise** primarily affects response times and confidence
3. **Learning effects** are evident across all models
4. **Face emotion** shows consistent effects on choice and confidence

The results support the hypothesis that participants use **predictive cues** to guide their responses, with **learning effects** modulating these relationships over time.

---

*Report generated on `r Sys.Date()`*  
*Analysis: CWT fMRI GLMM Study* 