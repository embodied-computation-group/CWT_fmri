---
title: "CWT fMRI Study - GLMM Analysis Report"
subtitle: "Predictive Processing in Emotion Recognition"
author: "CWT fMRI Analysis Team"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-tools: true
    fig-cap-location: bottom
    fig-dpi: 300
    css: styles.css
    include-in-header: header.html
execute:
  echo: true
  warning: false
  message: false
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(lme4)
library(knitr)
library(kableExtra)
library(DT)
library(plotly)

# Set global options
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = 6,
  dpi = 300,
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)

# Load pre-estimated models
accuracy_model <- readRDS("../../results/models/accuracy_model_simple.rds")
choice_model <- readRDS("../../results/models/choice_model_simple.rds")
rt_model <- readRDS("../../results/models/rt_model_simple.rds")
confidence_model <- readRDS("../../results/models/confidence_model_simple.rds")

# Load model summaries
model_summaries <- readLines("../../results/models/glmm_model_summaries.txt")
```

## Study Overview

This report presents the results of four Generalized Linear Mixed Models (GLMMs) examining predictive processing in emotion recognition using a confidence weighting task. The study involved **202 participants** completing **250 trials each** in a reversal learning paradigm.

### Key Experimental Design

- **Task**: Participants predicted emotional faces (Happy/Angry) based on visual cues
- **Paradigm**: Reversal learning with changing cue:stimulus associations
- **Manipulations**: Trial validity, stimulus noise, trials since reversal
- **Total Data**: 53,592 trials (48,199 after filtering)

### Model Overview

| Model | Dependent Variable | Distribution | Trials | Key Predictors |
|-------|-------------------|--------------|--------|----------------|
| **Accuracy** | Binary (correct/incorrect) | Binomial | High noise only | Trial validity, learning, face emotion |
| **Choice** | Face choice (Happy/Angry) | Binomial | High noise only | Signaled face, actual emotion, learning |
| **Response Time** | Response time (seconds) | Gamma | All trials | Stimulus noise, validity, learning |
| **Confidence** | Confidence rating (0-1) | Beta | All trials | Validity, noise, learning, emotion |

## Model Results

### 1. Accuracy Model

**Research Question**: How does trial validity and learning affect accuracy in high-noise trials?

```{r accuracy-model}
# Extract accuracy model statistics
accuracy_summary <- summary(accuracy_model)
accuracy_coef <- as.data.frame(accuracy_summary$coefficients$cond)
accuracy_coef$Parameter <- rownames(accuracy_coef)
accuracy_coef <- accuracy_coef[, c("Parameter", "Estimate", "Std. Error", "z value", "Pr(>|z|)")]

# Add significance stars
accuracy_coef$Significance <- case_when(
  accuracy_coef$`Pr(>|z|)` < 0.001 ~ "***",
  accuracy_coef$`Pr(>|z|)` < 0.01 ~ "**",
  accuracy_coef$`Pr(>|z|)` < 0.05 ~ "*",
  accuracy_coef$`Pr(>|z|)` < 0.1 ~ ".",
  TRUE ~ ""
)

# Format for display
accuracy_coef$Estimate <- round(accuracy_coef$Estimate, 4)
accuracy_coef$`Std. Error` <- round(accuracy_coef$`Std. Error`, 4)
accuracy_coef$`z value` <- round(accuracy_coef$`z value`, 3)
accuracy_coef$`Pr(>|z|)` <- format(accuracy_coef$`Pr(>|z|)`, scientific = TRUE, digits = 3)

# Display coefficient table
accuracy_coef %>%
  kable(
    caption = "Accuracy Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings**:
- **Trial Validity**: Valid trials show significantly higher accuracy (z = 5.03, p < 0.001)
- **Face Emotion**: Happy faces show lower accuracy than angry faces (z = -7.14, p < 0.001)
- **Learning**: Trial validity effects change with learning (interaction: z = 2.18, p = 0.030)

```{r accuracy-plot, fig.cap="Accuracy Model Predictions: Trial Validity × Trials Since Reversal × Face Emotion"}
# Include the existing accuracy plot
knitr::include_graphics("../../results/figures/glmm_models/glmm_accuracy_model.png")
```

### 2. Choice Model

**Research Question**: How do signaled faces and actual emotions influence choice behavior?

```{r choice-model}
# Extract choice model statistics
choice_summary <- summary(choice_model)
choice_coef <- as.data.frame(choice_summary$coefficients$cond)
choice_coef$Parameter <- rownames(choice_coef)
choice_coef <- choice_coef[, c("Parameter", "Estimate", "Std. Error", "z value", "Pr(>|z|)")]

# Add significance stars
choice_coef$Significance <- case_when(
  choice_coef$`Pr(>|z|)` < 0.001 ~ "***",
  choice_coef$`Pr(>|z|)` < 0.01 ~ "**",
  choice_coef$`Pr(>|z|)` < 0.05 ~ "*",
  choice_coef$`Pr(>|z|)` < 0.1 ~ ".",
  TRUE ~ ""
)

# Format for display
choice_coef$Estimate <- round(choice_coef$Estimate, 4)
choice_coef$`Std. Error` <- round(choice_coef$`Std. Error`, 4)
choice_coef$`z value` <- round(choice_coef$`z value`, 3)
choice_coef$`Pr(>|z|)` <- format(choice_coef$`Pr(>|z|)`, scientific = TRUE, digits = 3)

# Display coefficient table
choice_coef %>%
  kable(
    caption = "Choice Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings**:
- **Signaled Face**: Angry signaled faces reduce choice of angry (z = -5.23, p < 0.001)
- **Actual Emotion**: Happy faces strongly predict happy choices (z = 36.38, p < 0.001)
- **Learning**: Learning effects interact with signaled face (z = -4.10, p < 0.001)

```{r choice-plot, fig.cap="Choice Model Predictions: Signaled Face × Face Emotion × Trials Since Reversal"}
knitr::include_graphics("../../results/figures/glmm_models/glmm_choice_model.png")
```

### 3. Response Time Model

**Research Question**: How do stimulus noise and trial validity affect response times?

```{r rt-model}
# Extract RT model statistics
rt_summary <- summary(rt_model)
rt_coef <- as.data.frame(rt_summary$coefficients$cond)
rt_coef$Parameter <- rownames(rt_coef)
rt_coef <- rt_coef[, c("Parameter", "Estimate", "Std. Error", "z value", "Pr(>|z|)")]

# Add significance stars
rt_coef$Significance <- case_when(
  rt_coef$`Pr(>|z|)` < 0.001 ~ "***",
  rt_coef$`Pr(>|z|)` < 0.01 ~ "**",
  rt_coef$`Pr(>|z|)` < 0.05 ~ "*",
  rt_coef$`Pr(>|z|)` < 0.1 ~ ".",
  TRUE ~ ""
)

# Format for display
rt_coef$Estimate <- round(rt_coef$Estimate, 4)
rt_coef$`Std. Error` <- round(rt_coef$`Std. Error`, 4)
rt_coef$`z value` <- round(rt_coef$`z value`, 3)
rt_coef$`Pr(>|z|)` <- format(rt_coef$`Pr(>|z|)`, scientific = TRUE, digits = 3)

# Display coefficient table
rt_coef %>%
  kable(
    caption = "Response Time Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings**:
- **Stimulus Noise**: High noise trials show significantly longer RTs (z = 70.35, p < 0.001)
- **Trial Validity**: Invalid trials show shorter RTs (z = -5.87, p < 0.001)
- **Learning**: Validity effects change with learning (interaction: z = -3.26, p = 0.001)

```{r rt-plot, fig.cap="Response Time Model Predictions: Stimulus Noise × Trial Validity × Trials Since Reversal"}
knitr::include_graphics("../../results/figures/glmm_models/glmm_rt_model.png")
```

### 4. Confidence Model

**Research Question**: How do trial validity and stimulus noise affect confidence ratings?

```{r confidence-model}
# Extract confidence model statistics
conf_summary <- summary(confidence_model)
conf_coef <- as.data.frame(conf_summary$coefficients$cond)
conf_coef$Parameter <- rownames(conf_coef)
conf_coef <- conf_coef[, c("Parameter", "Estimate", "Std. Error", "z value", "Pr(>|z|)")]

# Add significance stars
conf_coef$Significance <- case_when(
  conf_coef$`Pr(>|z|)` < 0.001 ~ "***",
  conf_coef$`Pr(>|z|)` < 0.01 ~ "**",
  conf_coef$`Pr(>|z|)` < 0.05 ~ "*",
  conf_coef$`Pr(>|z|)` < 0.1 ~ ".",
  TRUE ~ ""
)

# Format for display
conf_coef$Estimate <- round(conf_coef$Estimate, 4)
conf_coef$`Std. Error` <- round(conf_coef$`Std. Error`, 4)
conf_coef$`z value` <- round(conf_coef$`z value`, 3)
conf_coef$`Pr(>|z|)` <- format(conf_coef$`Pr(>|z|)`, scientific = TRUE, digits = 3)

# Display coefficient table
conf_coef %>%
  kable(
    caption = "Confidence Model Coefficients",
    col.names = c("Parameter", "Estimate", "Std. Error", "z-value", "p-value", "Significance"),
    align = c("l", "r", "r", "r", "r", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  ) %>%
  column_spec(6, color = "red", bold = TRUE)
```

**Key Findings**:
- **Stimulus Noise**: High noise trials show significantly lower confidence (z = -127.10, p < 0.001)
- **Trial Validity**: Valid trials show higher confidence (z = 2.60, p = 0.009)
- **Face Emotion**: Happy faces show higher confidence (z = 18.35, p < 0.001)
- **Learning**: Validity effects change with learning (interaction: z = 3.50, p < 0.001)

```{r confidence-plot, fig.cap="Confidence Model Predictions: Trial Validity × Stimulus Noise × Trials Since Reversal"}
knitr::include_graphics("../../results/figures/glmm_models/glmm_confidence_model.png")
```

## Model Comparison

```{r model-comparison}
# Create model fit statistics table
fit_stats <- data.frame(
  Model = c("Accuracy Model", "Choice Model", "Response Time Model", "Confidence Model"),
  AIC = c(AIC(accuracy_model), AIC(choice_model), AIC(rt_model), AIC(confidence_model)),
  BIC = c(BIC(accuracy_model), BIC(choice_model), BIC(rt_model), BIC(confidence_model)),
  LogLik = c(logLik(accuracy_model), logLik(choice_model), logLik(rt_model), logLik(confidence_model)),
  N_Observations = c(nobs(accuracy_model), nobs(choice_model), nobs(rt_model), nobs(confidence_model)),
  N_Subjects = c(length(ranef(accuracy_model)$SubNo$`(Intercept)`), 
                length(ranef(choice_model)$SubNo$`(Intercept)`),
                length(ranef(rt_model)$SubNo$`(Intercept)`),
                length(ranef(confidence_model)$SubNo$`(Intercept)`))
)

# Format for display
fit_stats$AIC <- round(fit_stats$AIC, 2)
fit_stats$BIC <- round(fit_stats$BIC, 2)
fit_stats$LogLik <- round(fit_stats$LogLik, 2)

# Display fit statistics
fit_stats %>%
  kable(
    caption = "Model Fit Statistics",
    col.names = c("Model", "AIC", "BIC", "Log Likelihood", "N Observations", "N Subjects"),
    align = c("l", "r", "r", "r", "r", "r")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE,
    font_size = 12
  )
```

## Key Findings Summary

### Main Effects

::: {.panel-tabset}

#### Trial Validity
- **Accuracy**: Valid trials show significantly higher accuracy
- **Confidence**: Valid trials show higher confidence ratings
- **Response Time**: Invalid trials show shorter response times

#### Stimulus Noise
- **Response Time**: High noise trials show significantly longer RTs
- **Confidence**: High noise trials show significantly lower confidence

#### Face Emotion
- **Accuracy**: Happy faces show lower accuracy than angry faces
- **Choice**: Happy faces strongly predict happy choices
- **Confidence**: Happy faces show higher confidence ratings

#### Learning Effects
- **Accuracy**: Trial validity effects change with learning
- **Choice**: Learning interacts with signaled face effects
- **Response Time**: Validity effects change with learning
- **Confidence**: Validity effects change with learning

:::

### Interaction Effects

::: {.panel-tabset}

#### Validity × Learning
- **Accuracy**: Trial validity effects change with learning (p = 0.030)
- **Choice**: Learning interacts with signaled face effects (p < 0.001)
- **Response Time**: Validity effects change with learning (p = 0.001)
- **Confidence**: Validity effects change with learning (p < 0.001)

#### Noise × Validity
- **Response Time**: Noise effects interact with trial validity
- **Confidence**: Noise effects interact with trial validity

#### Emotion × Validity
- **Accuracy**: Different patterns for happy vs angry faces
- **Choice**: Emotion effects interact with signaled face

:::

## Complete Model Outputs

::: {.callout-note}
## Full Model Summaries

The complete model summaries are provided below for reference.

```{r full-summaries, echo=FALSE}
cat(paste(model_summaries, collapse = "\n"))
```

:::

## Conclusions

This analysis reveals robust evidence for predictive processing in emotion recognition:

1. **Trial validity** consistently affects all dependent measures
2. **Stimulus noise** primarily affects response times and confidence
3. **Learning effects** are evident across all models
4. **Face emotion** shows consistent effects on choice and confidence

The results support the hypothesis that participants use predictive cues to guide their responses, with learning effects modulating these relationships over time.

---

*Report generated on `r Sys.Date()` using Quarto* 